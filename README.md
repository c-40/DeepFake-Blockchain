1. Video Loading: Library that can read multiple video formats like OpenCV is utilized for loading the video file. Sequentially, each frame is removed for further processing.

2. Frame Extraction: At regular intervals, frames are extracted. The cv2.VideoCapture() function in OpenCV is used for this, and read() then gets utilized to read each frame.

3. Frame Preprocessing: This procedure entails downsizing frames to the 224x224 input size that the CNN model requires. Additionally, pixel values are scaled between 0 and 1 to normalize frames. To improve model resilience, image augmentation mechanisms such as flipping, rotation, or brightness modifications are used.

4. Face Detection and Alignment: Each frame's significant characteristics, including the mouth, nose, and eyes, are identified by Dlib's pre-trained facial landmark detector. High accuracy has been provided by its face detection models based on convolutional neural networks (CNN) and histogram of oriented gradients (HOG). Bounding boxes produced by the detection model will be utilized to extract the face region after it has been identified. These bounding box coordinates aid in resizing individual faces for further processing. Key facial features like the eyes and nose are used to align recognized faces in order to guarantee correctness. This makes the faces more consistent for deepfake detection models by correcting any tilt or rotation.

5. Feature Extraction: The CNN collects spatial features from every frame, identifying key facial features, while the LSTM analyzes the changes in time across the frame sequence. These characteristics are employed by the model to categorize data.In order to distinguish between authentic and influenced footage,feature extraction for deepfake detection entails identifying and eliminating relevant trends from videos.Spatial feature extraction and temporal feature extraction are the two key phases in the procedure.
Using Convolutional Neural Networks (CNNs) to extract spatial data from individual video frames is the first step in the method. CNNs identify edges, textures, and more intricate structures like skin texture,features of the face,and fine-grained details by sliding filters over the picture. ResNe along with other well-known architectures have been utilized to carry out this extraction. Complex characteristics that are frequently seen  in edited films, such pixel level differences, inconsistent lighting and face motions are captured by the network's convolutional layers. 
Temporal Feature Extraction: Long Short Term Memory (LSTM) networks are used to capture temporal dynamics, or variances between frames. Recurrent neural networks (RNNs) of the LSTM type are ideal for processing video frames over time since they are intended to handle sequential inputs. These networks identify abnormal temporal transitions between frames, lip synchronization and facial movement irregularities. LSTMs assist in detecting minute temporal variations that can suggest a video has been altered, including sudden shifts in the way facial features move or background artifacts. When combined, these feature extraction techniques, temporal characteristics from LSTMs and spatial features from CNNs are essential for precisely distinguishing real movies from deepfake ones.

6. Prediction: The deep learning model suggests if the video has updated or authentic content based on an analysis of the gathered attributes. The outcomes include an identification label (real/fake) and a confidence score.
